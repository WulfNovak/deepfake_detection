{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\WulfN\\\\.cache\\\\huggingface\\\\hub\\\\\")\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "# download model\n",
    "# hf_hub_download(repo_id = 'prithivMLmods/Deep-Fake-Detector-Model', filename = 'config.json') # presumably the config file is needed first\n",
    "# model.safetensors, # pytorch_model.bin\n",
    "\n",
    "# Datasets can be found here: https://huggingface.co/datasets?modality=modality:image&sort=trending&search=deepfake\n",
    "\n",
    "### Load Data\n",
    "dataset = load_dataset(\"Whab/deepfake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "# import ipyplot\n",
    "#from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# preview_num = 5\n",
    "\n",
    "# sample_images = dataset['train'][1:preview_num + 1]\n",
    "\n",
    "# display_images = [\n",
    "#     widgets.Image(value=sample_images['image'][i].tobytes(), format='png', width=256, height=256) \n",
    "#     for i in range(preview_num)\n",
    "# ]\n",
    "\n",
    "# display_images\n",
    "# display(widgets.HBox(display_images))\n",
    "\n",
    "## Display images vertically\n",
    "# for i in range(preview_num):\n",
    "#     display(sample_images['image'][i])\n",
    "\n",
    "# display(display_images) \n",
    "# #widgets.HBox(display_images)\n",
    "# # hbox = widgets.HBox(display_images)\n",
    "# # display(hbox)\n",
    "# #display_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create batches of image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "# import modin as pd\n",
    "\n",
    "def create_image_batches(data, batch_size):\n",
    "    \n",
    "    num_images = data.num_rows # data[0].num_rows may be needed when using starmap\n",
    "    batch_size = batch_size\n",
    "\n",
    "    # index value of first image in batch i\n",
    "    batch_key = list(range(0, num_images, batch_size))\n",
    "\n",
    "    # initializing result dataframe\n",
    "    result = pd.DataFrame({'index': [],\n",
    "                            'image_batch':[]})\n",
    "\n",
    "    # batching images in groups of 3\n",
    "    for i in range(len(batch_key)):\n",
    "\n",
    "        # index of all images in batch\n",
    "        batch_indices = list(range(batch_key[i], batch_key[i] + batch_size - 1))\n",
    "\n",
    "        # appending images from batch into list\n",
    "        batched_images = []\n",
    "        for j in batch_indices:\n",
    "            batched_images.append(chunks[0][j]['image'])\n",
    "\n",
    "        batch = pd.DataFrame({'index': [i],\n",
    "                            'image_batch': [batched_images]})\n",
    "        \n",
    "        # Adding batch to dataframe\n",
    "        result = pd.concat([result, batch])\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Function to do parallel processes over chunks of dataframe\n",
    "def parallel_apply(func, args, n_cores=None):\n",
    "    \n",
    "    if n_cores is None:\n",
    "        n_cores = cpu_count()\n",
    "\n",
    "    # Apply function on partitions given arguments\n",
    "    with Pool(n_cores) as pool:\n",
    "        result = pd.concat(pool.starmap(func, args), chunksize=n_cores) # create_image_batches(batch_size=3)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = cpu_count()\n",
    "\n",
    "def multiply_and_add(x, y):\n",
    "    \"\"\"\n",
    "    Multiply x and y, then add y.\n",
    "    \"\"\"\n",
    "    return x * y + y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List of argument pairs (tuples) for the function\n",
    "    argument_pairs = [(1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]\n",
    "\n",
    "    try:\n",
    "        with Pool(n_cores) as pool:\n",
    "            results = pool.starmap(multiply_and_add, argument_pairs)\n",
    "    finally: \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # Output the results\n",
    "print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Need to parallelize this process\n",
    "n_cores = cpu_count()\n",
    "# Test data\n",
    "test_df = dataset['train'].train_test_split(test_size=0.1, shuffle=False)\n",
    "split_df1 = test_df['test'].shard(num_shards=n_cores, index=0)\n",
    "#split_df2 = test_df['test'].shard(num_shards=n_cores, index=1)\n",
    "#split_df1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create arguments for starmap\n",
    "    \n",
    "    chunks = [test_df['test'].shard(num_shards=n_cores, index=i) for i in range(n_cores)] \n",
    "    args = []\n",
    "\n",
    "    for i in range(n_cores):\n",
    "        args.append((chunks[i], 3))\n",
    "    \n",
    "    with Pool(n_cores) as pool:\n",
    "        result = pool.starmap(create_image_batches, args) # chunks may need to be chunks[index] value\n",
    "\n",
    "pool.close()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: Map create_image_batches over each chunk with batch_size of 3\n",
    "# with Pool(n_cores) as pool:\n",
    "#     chunks = [test_df['test'].shard(num_shards=n_cores, index=i) for i in range(n_cores)] \n",
    "#     args = (chunks, 3)\n",
    "\n",
    "pool = Pool(n_cores)#with Pool(n_cores) as pool:\n",
    "chunks = [test_df['test'].shard(num_shards=n_cores, index=i) for i in range(n_cores)] \n",
    "\n",
    "# Create arguments for starmap\n",
    "args = []\n",
    "for i in range(n_cores):\n",
    "    args.append((chunks[i], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df_length = 100\n",
    "\n",
    "# Rather than pipe(image i), pipe(batch of images i), try batch of 3 images then 2\n",
    "# predictions = pd.DataFrame()\n",
    "# for i in range(batches):\n",
    "#     image_score = pd.DataFrame(pipe(batches[i])) #dataset['train'][:df_length+1]['image'][i]\n",
    "#     predictions = pd.concat([predictions, pd.concat({i: image_score}, names=['image_num'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Consider Fully Sharded Data Parallel on model for processing\n",
    "\n",
    "batches_test = batches[:100]\n",
    "\n",
    "# Huggingface model\n",
    "pipe = pipeline(\"image-classification\", model=\"prithivMLmods/Deep-Fake-Detector-Model\", device=-1); \n",
    "\n",
    "# Get predictions over batches of images\n",
    "predictions_test = parallel_apply(batches_test, 'image_batch', pipe)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table in repo\n",
    "import pickle\n",
    "predictions.to_pickle('huggingface_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist'n of fake and real scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist'n of fake and real scores\n",
    "import seaborn as sb\n",
    "sb.displot(kind='hist', data=predictions.loc[lambda x: x.label == 'Fake'], bins=100)\n",
    "sb.displot(kind='hist', data=predictions.loc[lambda x: x.label == 'Real'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
