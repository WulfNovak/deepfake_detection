{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from huggingface_hub import hf_hub_download\n",
    "from datasets import load_dataset\n",
    "import os \n",
    "os.chdir(\"C:\\\\Users\\\\WulfN\\\\.cache\\\\huggingface\\\\hub\\\\\")\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "# download model\n",
    "#hf_hub_download(repo_id = 'prithivMLmods/Deep-Fake-Detector-Model', filename = 'config.json') # presumably the config file is needed first\n",
    "# model.safetensors, # pytorch_model.bin\n",
    "\n",
    "# Datasets can be found here: https://huggingface.co/datasets?modality=modality:image&sort=trending&search=deepfake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data\n",
    "dataset = load_dataset(\"Whab/deepfake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preview dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "# import ipyplot\n",
    "#from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# preview_num = 5\n",
    "\n",
    "# sample_images = dataset['train'][1:preview_num + 1]\n",
    "\n",
    "# display_images = [\n",
    "#     widgets.Image(value=sample_images['image'][i].tobytes(), format='png', width=256, height=256) \n",
    "#     for i in range(preview_num)\n",
    "# ]\n",
    "\n",
    "# display_images\n",
    "# display(widgets.HBox(display_images))\n",
    "\n",
    "## Display images vertically\n",
    "# for i in range(preview_num):\n",
    "#     display(sample_images['image'][i])\n",
    "\n",
    "# display(display_images) \n",
    "# #widgets.HBox(display_images)\n",
    "# # hbox = widgets.HBox(display_images)\n",
    "# # display(hbox)\n",
    "# #display_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reate batches of image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "# import modin as pd\n",
    "\n",
    "def create_image_batches(data, batch_size):\n",
    "    \n",
    "    num_images = data.num_rows # data[0].num_rows may be needed when using starmap\n",
    "    batch_size = batch_size\n",
    "    # index value of first image in batch i\n",
    "    batch_key = list(range(0, num_images, batch_size))\n",
    "    \n",
    "    result = pd.DataFrame({'index': [],\n",
    "                           'image_batch':[]})\n",
    "    \n",
    "    for i in range(len(batch_key)):\n",
    "\n",
    "        # index of all images in batch\n",
    "        batch_indices = list(range(batch_key[i], batch_key[i] + batch_size))\n",
    "        # condition such that the last batch indice + batch_size < num_images\n",
    "\n",
    "        # appending images from batch into list\n",
    "        batch_i = []\n",
    "        for j in batch_indices:\n",
    "            while batch_indices[j] < num_images:\n",
    "                add_image = data[j]['image']\n",
    "                batch_i.append(add_image)\n",
    "\n",
    "        batch = pd.DataFrame({'index': [i],\n",
    "                              'image_batch': [batch_i]})\n",
    "        \n",
    "        # Adding batch to dataframe\n",
    "        result = pd.concat([result, batch])\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Function to do parallel processes over chunks of dataframe\n",
    "def parallel_apply(func, args, n_cores=None):\n",
    "    \n",
    "    if n_cores is None:\n",
    "        n_cores = cpu_count()\n",
    "\n",
    "    # Apply function on partitions given arguments\n",
    "    with Pool(n_cores) as pool:\n",
    "        result = pd.concat(pool.starmap(func, args), chunksize=n_cores) # create_image_batches(batch_size=3)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to parallelize this process\n",
    "n_cores = cpu_count()\n",
    "# Test data\n",
    "test_df = dataset['train'].train_test_split(test_size=0.1, shuffle=False)\n",
    "split_df1 = test_df['test'].shard(num_shards=n_cores, index=0)\n",
    "#split_df2 = test_df['test'].shard(num_shards=n_cores, index=1)\n",
    "split_df1\n",
    "split_df2\n",
    "\n",
    "#split_df1['image'][0]\n",
    "#split_df2['image'][0]\n",
    "#result = parallel_apply(test_data, func = create_image_batches) # how to have chunk passed to create_image_batches\n",
    "#batches = create_image_batches(dataset, batch_size = 3)\n",
    "\n",
    "# Consider Fully Sharded Data Parallel on model for processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Goal: Map create_image_batches over each chunk with batch_size of 3\n",
    "with Pool(n_cores) as pool:\n",
    "    # batch size is 3\n",
    "    chunks = [test_df['test'].shard(num_shards=n_cores, index=i) for i in range(n_cores)] \n",
    "    args = (chunks, 3)\n",
    "    # chunks = [(test_df['test'].shard(num_shards=n_cores, index=i), 3) for i in range(n_cores)] \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#chunks[0].num_rows\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_image_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m result\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#data[j]['image']\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#chunks[0][2242]['image']\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[102], line 27\u001b[0m, in \u001b[0;36mcreate_image_batches\u001b[1;34m(data, batch_size)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m batch_indices:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m batch_indices[j] \u001b[38;5;241m<\u001b[39m num_images:\n\u001b[1;32m---> 27\u001b[0m         add_image \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m         batch_i\u001b[38;5;241m.\u001b[39mappend(add_image)\n\u001b[0;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: [i],\n\u001b[0;32m     31\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_batch\u001b[39m\u001b[38;5;124m'\u001b[39m: [batch_i]})\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\arrow_dataset.py:2747\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2746\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2747\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:403\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:444\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    443\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m--> 444\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_features_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:222\u001b[0m, in \u001b[0;36mPythonFeaturesDecoder.decode_row\u001b[1;34m(self, row)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\features\\features.py:2045\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[1;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[0;32m   2030\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2031\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   2032\u001b[0m \n\u001b[0;32m   2033\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;124;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m-> 2045\u001b[0m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2046\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   2047\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   2048\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   2049\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   2050\u001b[0m         )\n\u001b[0;32m   2051\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\features\\features.py:1405\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image, Video)):\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m schema\u001b[38;5;241m.\u001b[39mdecode:\n\u001b[1;32m-> 1405\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\datasets\\features\\image.py:188\u001b[0m, in \u001b[0;36mImage.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(BytesIO(bytes_))\n\u001b[1;32m--> 188\u001b[0m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# to avoid \"Too many open files\" errors\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mgetexif()\u001b[38;5;241m.\u001b[39mget(PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mExifTags\u001b[38;5;241m.\u001b[39mBase\u001b[38;5;241m.\u001b[39mOrientation) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     image \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImageOps\u001b[38;5;241m.\u001b[39mexif_transpose(image)\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "#chunks[0].num_rows\n",
    "result = create_image_batches(chunks[0], batch_size = 3)\n",
    "\n",
    "result # insanely slow\n",
    "# is batching the images worthwhile\n",
    "\n",
    "# may be better to focus on sharding the model across threads\n",
    "\n",
    "#data[j]['image']\n",
    "#chunks[0][2242]['image']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pool not running",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_image_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#for result in pool.starmap(create_image_batches, chunks, chunksize=n_cores):\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\multiprocessing\\pool.py:382\u001b[0m, in \u001b[0;36mPool.starmap_async\u001b[1;34m(self, func, iterable, chunksize, callback, error_callback)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    378\u001b[0m         error_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    379\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m    Asynchronous version of `starmap()` method.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\multiprocessing\\pool.py:476\u001b[0m, in \u001b[0;36mPool._map_async\u001b[1;34m(self, func, iterable, mapper, chunksize, callback, error_callback)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_map_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, mapper, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    472\u001b[0m         error_callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    Helper function to implement map, starmap and their async counterparts.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(iterable, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__len__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    478\u001b[0m         iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iterable)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\multiprocessing\\pool.py:353\u001b[0m, in \u001b[0;36mPool._check_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m RUN:\n\u001b[1;32m--> 353\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPool not running\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Pool not running"
     ]
    }
   ],
   "source": [
    "result = pool.starmap(create_image_batches, chunks, chunksize=n_cores) # chunks may need to be chunks[index] value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#df_length = 100\n",
    "\n",
    "# Rather than pipe(image i), pipe(batch of images i), try batch of 3 images then 2\n",
    "# predictions = pd.DataFrame()\n",
    "# for i in range(batches):\n",
    "#     image_score = pd.DataFrame(pipe(batches[i])) #dataset['train'][:df_length+1]['image'][i]\n",
    "#     predictions = pd.concat([predictions, pd.concat({i: image_score}, names=['image_num'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprithivMLmods/Deep-Fake-Detector-Model\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m); \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get predictions over batches of images\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m predictions_test \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatches_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_batch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m predictions_test\n",
      "Cell \u001b[1;32mIn[105], line 17\u001b[0m, in \u001b[0;36mparallel_apply\u001b[1;34m(df, var, func, n_cores)\u001b[0m\n\u001b[0;32m     15\u001b[0m chunks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(df, n_cores)  \u001b[38;5;66;03m# Split DataFrame into chunks\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(n_cores) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m---> 17\u001b[0m     result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(pool\u001b[38;5;241m.\u001b[39mmap(func, \u001b[43mchunks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m)) \u001b[38;5;66;03m# need to include index in here too\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Consider Fully Sharded Data Parallel on model for processing\n",
    "\n",
    "batches_test = batches[:100]\n",
    "\n",
    "# Huggingface model\n",
    "pipe = pipeline(\"image-classification\", model=\"prithivMLmods/Deep-Fake-Detector-Model\", device=-1); \n",
    "\n",
    "# Get predictions over batches of images\n",
    "predictions_test = parallel_apply(batches_test, 'image_batch', pipe)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "                           ...                        \n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "0    [<PIL.JpegImagePlugin.JpegImageFile image mode...\n",
       "Name: image_batch, Length: 100, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = 'image_batch'\n",
    "batches_test[var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table in repo\n",
    "import pickle\n",
    "predictions.to_pickle('huggingface_predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dist'n of fake and real scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist'n of fake and real scores\n",
    "import seaborn as sb\n",
    "sb.displot(kind='hist', data=predictions.loc[lambda x: x.label == 'Fake'], bins=100)\n",
    "sb.displot(kind='hist', data=predictions.loc[lambda x: x.label == 'Real'], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
